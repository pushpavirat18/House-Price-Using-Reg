## ğŸ¡ Forecasting House Prices Accurately Using Smart Regression Techniques in Data Science

> **Project Type**: Regression Problem
> **Dataset**: [Kaggle - House Prices: Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)
> **Tech Stack**: Python, scikit-learn, LightGBM, XGBoost, Streamlit
> **Use Case**: Accurate house price prediction using statistical and ensemble ML techniques

---

### ğŸ§  Project Summary

This project presents a **smart regression model** to accurately forecast house prices based on various property characteristics. We leverage a blend of classical linear models (Ridge, Lasso) and powerful tree-based models (XGBoost, LightGBM), combined with extensive feature engineering and data cleaning techniques to produce highly reliable results.

The final model is deployed as a **Streamlit web app**, where users can input home features and get a real-time price prediction!

---

### âœ¨ Key Features

* ğŸ“Š Interactive **Exploratory Data Analysis** with visual insights
* ğŸ› ï¸ Advanced **Data Cleaning & Feature Engineering**
* ğŸ§ª Evaluated multiple regression models using **RMSE**
* ğŸ† Final model selection based on cross-validation
* ğŸŒ **Streamlit App** for real-time prediction and GUI
* ğŸ’¾ Trained models saved and reused for fast prediction

---

### ğŸ“‚ Project Structure

```
HPP-GUI/
â”œâ”€â”€ app.py                    # Streamlit application
â””â”€â”€ data/
    â”œâ”€â”€ train.csv
    â”œâ”€â”€ test.csv
â””â”€â”€ models/
    â”œâ”€â”€ ridge_model.pkl          # Final trained RidgeCV model
    â”œâ”€â”€ top_features.pkl         # List of top features used
    â”œâ”€â”€ all_features.pkl         # All one-hot encoded feature columns
â”œâ”€â”€ result.csv           # Sample output predictions
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ HousePricePrediction.ipynb  # Full analysis & model training
```

---

### ğŸ”§ How It Works

1. **Preprocessing & Cleaning**

   * Fill missing values (median for numeric, "None" for categorical)
   * Feature engineering: `TotalSF`, `TotalBath`, log transformations
   * One-hot encoding for categoricals
2. **Modeling**

   * Linear models: RidgeCV, LassoCV
   * Tree models: LightGBM, XGBoost
   * Evaluation: RMSE on log-transformed prices
3. **Deployment**

   * Final model: RidgeCV
   * Streamlit GUI with top 15 features
   * Predicts and displays actual price using `np.expm1(log_price)`

---

### ğŸš€ How to Run Locally

#### ğŸ“¥ 1. Clone the Repo

```bash
git clone https://github.com/MTS-Krishna/HPP-GUI.git
cd HPP-GUI
```

#### ğŸ“¦ 2. Install Dependencies

```bash
pip install -r requirements.txt
```

#### ğŸ¯ 3. Run the App

```bash
streamlit run app.py
```

> The app usually runs at: [http://localhost:8501](http://localhost:8501)

---

### ğŸ›  Requirements

```
numpy
pandas
matplotlib
seaborn
scikit-learn
xgboost
lightgbm
streamlit
joblib
```

(Already listed in `requirements.txt`)

---

### ğŸ“ˆ Model Performance

| Model    | RMSE (Log Scale)     |
| -------- | -------------------- |
| RidgeCV  | âœ… Best (lowest RMSE) |
| LassoCV  | Slightly worse       |
| XGBoost  | Competitive          |
| LightGBM | Competitive          |

---

### ğŸŒ Deployment Options

* âœ… Run locally with Streamlit
* â˜ï¸ Deploy to [Streamlit Cloud](https://streamlit.io/cloud)
* ğŸ“¦ Optionally dockerize for flexible deployment

---

### ğŸ¤ Acknowledgements

* ğŸ“Š [Kaggle: House Prices Dataset](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)
* ğŸ’¡ Inspiration & guidance from Data Science mentors and open-source contributors

---
